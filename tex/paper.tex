\documentclass[twocolumn,superscriptaddress,aps]{revtex4-1}

\usepackage[utf8]{inputenc}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{bm}
\usepackage{cancel}
\usepackage{bbold}
\usepackage{slashed}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}
\newcommand{\kcnote}[1]{\textcolor{red}{[KC: #1]}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}


% ==============================================================================

\title{\Large{Adversarial Variational Optimization of Non-Differentiable Simulators}}
\vspace{1cm}
\author{\small{\bf Gilles Louppe}\thanks{\texttt{g.louppe@nyu.edu}}}
\affiliation{New York University}
\author{\small{\bf Kyle Cranmer}\thanks{\texttt{kyle.cranmer@nyu.edu}}}
\affiliation{New York University}

\begin{abstract}

Complex computer simulators are increasingly used across fields of science to
describe generative models tying parameters of an underlying theory to
experimental observations. Inference in this setup is often
difficult, as simulators rarely provide a way to directly evaluate the likehood
function for a given observation. In this note, we develop a likelihood-free
inference algorithm for fitting a forward non-differentiable generative model to
observed data. We adapt the adversarial training procedure of generative
adversarial networks by replacing the implicit generative network with a
domain-based scientific simulator, and solve the resulting non-differentiable
minimax problem by minimizing variationial upper bounds of the adversarial objectives.
Effectively, the procedure results in learning an arbitrarily tight
proposal distribution over simulator parameters, such that the corresponding
marginal distribution of the generated data matches the observations.
\glnote{Mention experimental results.}
\glnote{Add 'so what?' conclusion.}


\end{abstract}

\maketitle

% ==============================================================================

\section{Introduction}

In many fields of science such as particle physics, climatology or population
genetics, computer simulators are used to describe complex processes that tie
parameters of an underlying theory to high dimensional observations. In most
cases, these implicit generative models~\citep{2016arXiv161003483M} are
specified as imperative implementations of forward stochastic processes that
generate data. Because it is usually computationally intractable, most simulators
do not provide a way to directly evaluate the likelihood function for a
given observation, thereby making inference difficult.

In this note, we develop a likelihood-free inference algorithm for the point
estimation from observed data of the parameters of a forward non-differentiable
generative model. We propose to adapt the adversarial
training procedure of generative adversarial
networks~\cite{goodfellow2014generative} by replacing the implicit generative
network with a domain-based scientific simulator, and solve the resulting
non-differentiable minimax problem by minimizing variational upper
bounds~\citep{2011arXiv1106.4487W,2012arXiv1212.4507S} of the adversarial
objectives. The procedure results in learning a proposal distribution over
simulator parameters, hence producing an abitrarily tight family of models whose
joint collection of generated samples matches the observed data.


% ==============================================================================

\section{Problem statement}
\label{sec:problem}

We consider a family of parameterized densities $p_\theta(\mathbf{x})$
defined implicitly through the simulation of a stochastic generative process,
where $\mathbf{x} \in \mathbb{R}^d$ is the data and $\theta$ are the
parameters of interest. The simulation may involve some complicated latent
process, such that
\begin{equation}\label{eqn:p_x}
    p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x}|\mathbf{z}) p_{z(\theta)}(\mathbf{z}) d\mathbf{z}
\end{equation}
where $\mathbf{z} \in {\cal Z}$ is a latent variable providing an external
source of randomness. In particular, $\mathbf{z}$ is not necessarily assumed to
be a fixed-size vector (e.g., it can be a sequence of variable length) and its
distribution $p_{z(\theta)}$ may itself depend on $\theta$ in some intricated way.

We assume that we already have an accurate simulation of the stochastic
generative process that defines $p_\theta(\mathbf{x}|\mathbf{z})$, as
specified through a highly regularized deterministic function $g(\cdot; \theta) : {\cal Z} \to
\mathbb{R}^d$ with usually few parameters. That is, we consider
\begin{equation}\label{eqn:p_theta}
    \mathbf{x} \sim p_\theta \equiv \mathbf{z} \sim p_{z(\theta)}, \mathbf{x} = g(\mathbf{z}; \theta)
\end{equation}
such that the likelihood $p_\theta(\mathbf{x})$ can be rewritten as
\begin{equation}\label{eqn:p_x_sim}
    p_\theta(\mathbf{x}) = \frac{\partial}{\partial x_1} \dots \frac{\partial}{\partial x_d} \int_{\{\mathbf{z}:g(\mathbf{z};\theta) \leq \mathbf{x}\}} p_{z(\theta)}(\mathbf{z}) d\mathbf{z}.
\end{equation}
\glnote{Is it correct to write $d\mathbf{z}$ given the undefined structure of ${\cal Z}$?}
Importantly, the simulator $g$ is assumed to be a non-invertible function, that can only be
used to generate data in forward mode. For this reason, evaluating the integral
in Eqn.~\ref{eqn:p_x_sim} is intractable. As commonly found
in science, we finally assume the lack of access to or existence of derivatives of $g$ with respect to $\theta$,
e.g. as when $g$ is specified as a computer program.

Given some observed data $\{ \mathbf{x}_i | i=1, \dots, N \}$ drawn from the
(unknown) true distribution $p_r$, our goal is the inference of the parameters
of interest $\theta^*$ that minimize the divergence between $p_r$ and
the modeled data distribution $p_\theta$ induced by $g(\cdot;
\theta)$ over $\mathbf{z}$. That is,
\begin{equation}
    \theta^* = \arg \min_{\theta \in \Theta} \rho(p_r, p_\theta),
\end{equation}
where $\rho$ is some distance or divergence.


% ==============================================================================

\section{Background}

\subsection{Generative adversarial networks}
\label{sec:gans}

Generative adversarial networks (GANs) were first proposed by
\cite{goodfellow2014generative} as a way to build an implicit generative model
capable of producing samples from random noise $\mathbf{z}$. More specifically,
a generative model $g(\cdot; \theta)$ is pit against an adversarial
classifier $d(\cdot; \phi):\mathbb{R}^d \to [0,1]$ with parameters $\phi$ and whose antagonistic objective is to recognize real data $\mathbf{x}$
from generated data $\tilde{\mathbf{x}} = g(\mathbf{z}; \theta)$. Both models $g$ and $d$
are trained simultaneously, in such a way that $g$ learns to fool
its adversary $d$ (which happens when $g$ produces samples comparable to the
observed data), while $d$ continuously adapts to changes in $g$. When $d$ is
trained to optimality before each parameter update of the generator, it can
be shown that the original adversarial learning procedure amounts to minimizing
the Jensen-Shannon divergence $\text{JSD}(p_r \parallel p_\theta)$ between $p_r$ and $p_\theta$.

As thoroughly explored in \citep{2017arXiv170104862A}, GANs remain remarkably
difficult to train because of vanishing gradients as $d$ saturates, or because of
unreliable updates when the training procedure is relaxed. As a remedy,
Wasserstein GANs~\citep{2017arXiv170107875A} reformulate the adversarial
setup in order to minimize the Wasserstein-1 distance $W(p_r, p_\theta)$ by
replacing the adversarial classifier with a 1-Lipschitz adversarial critic
$d(\cdot; \phi) : \mathbb{R}^d \to \mathbb{R}$. Under the WGAN-GP formulation of \cite{2017arXiv170400028G}
for stabilizing the optimization procedure,
training $d$ and $g$ results in alternating gradient updates on $\phi$ and $\theta$ in order to respectively minimize
\begin{align}
    {\cal L}_d =\,& \mathbb{E}_{\tilde{\mathbf{x}} \sim p_\theta} [d(\tilde{\mathbf{x}};\phi)] - \mathbb{E}_{\mathbf{x} \sim p_r} [d(\mathbf{x};\phi)]  \nonumber \\
                  & + \lambda \mathbb{E}_{\hat{\mathbf{x}} \sim p_{\hat{\mathbf{x}}}} [(|| \nabla_{\hat{\mathbf{x}}} d({\hat{\mathbf{x}}};\phi) ||_2 - 1)^2] \\
    {\cal L}_g =\,& -\mathbb{E}_{\tilde{\mathbf{x}} \sim p_\theta} [d(\tilde{\mathbf{x}};\phi)]
\end{align}
where ${\hat{\mathbf{x}}} := \epsilon \mathbf{x} + (1-\epsilon)\tilde{\mathbf{x}}$, for $\epsilon \sim U[0,1]$, $\mathbf{x} \sim p_r$ and $\tilde{\mathbf{x}} \sim p_\theta$.


\subsection{Variational optimization}

Evolution strategies~\citep{2011arXiv1106.4487W} and variational optimization~\cite{2012arXiv1212.4507S} are general
optimization techniques that can be used to form a differentiable bound
on the optima of a non-differentiable function. Given a function $f$ to minimize,
these techniques are based on the simple fact that
\begin{equation}
    \min_{\theta \in \Theta} f(\theta) \leq \mathbb{E}_{\theta \sim q_\psi(\theta)} [f(\theta)] = U(\psi),
\end{equation}
where $q_\psi$ is a proposal distribution with parameters $\psi$ over input values $\theta$.
That is, the minimum of a set of function values is always less than or equal
to any of their average. Provided that the proposal is flexible enough, the parameters $\psi$
can be updated to place its mass arbitrarily tight around the optimum $\theta^* = \min_{\theta \in \Theta} f(\theta)$.

Under mild restrictions outlined in  \citep{2012arXiv1212.4507S}, the bound
$U(\psi)$ is differentiable with respect to $\psi$, and using the log-likelihood
trick it comes:
\begin{align}\label{eqn:approx-grad}
    \nabla_\psi U(\psi) &= \nabla_\psi \mathbb{E}_{\theta \sim q_\psi(\theta)} [f(\theta)] \nonumber \\
    &= \nabla_\psi \int f(\theta)  q_\psi(\theta)  d\theta \nonumber \\
    &= \int f(\theta) \nabla_\psi q_\psi(\theta)  d\theta \nonumber \\
    &= \int \left[ f(\theta) \nabla_\psi \log q_\psi(\theta) \right]  q_\psi(\theta)  d\theta \nonumber \\
    &= \mathbb{E}_{\theta \sim q_\psi(\theta)} [f(\theta) \nabla_\psi \log q_\psi(\theta)]
\end{align}
Effectively, this means that provided that the score function $\nabla_\psi \log
q_\psi(\theta)$ of the proposal is known and that one can evaluate
$f(\mathbf{\theta})$ for any $\theta$, then one can construct empirical
estimates of Eqn.~\ref{eqn:approx-grad}, which can in turn be used to minimize
$U(\psi)$ with stochastic gradient descent (or a variant thereof, like
Adam~\cite{2014arXiv1412.6980K} or the Natural Evolution Strategy
algorithm~\citep{2011arXiv1106.4487W}, for scaling invariance and
robustness to noisy gradients).

% In this simple form, stochastic gradient descent based on
% Eqn~\ref{eqn:approx-grad} is known to suffer from premature convergence and lack
% of scale invariance. To avoid the dependence on the parameterization of the
% proposal distribution, natural gradients~\citep{amari1998natural} can be used
% instead to point towards the steepest descent in the space of realizable
% proposal distributions, rather than in the space of their parameters. Effectively,
% the natural evolution strategy (NES) algorithm~\citep{2011arXiv1106.4487W}
% corrects the gradient of $U$ according to the local curvature of the KL-divergence
% surface of proposal distributions, which results in forming
% \begin{equation}\label{eqn:approx-grad-nat}
%     \tilde{\nabla}_\psi U(\psi) = \mathbf{F}(\psi)^{-1} \nabla_\psi U(\psi),
% \end{equation}
% where $\mathbf{F}(\psi)$ is the Fisher information matrix
% \begin{equation}
%     \mathbf{F}(\psi) = \mathbb{E}_{\theta \sim q_\phi}[ \nabla_\psi \log q_\psi(\theta) \nabla_\psi \log q_\psi(\theta)^{\top} ]
% \end{equation}
% and can be estimated from samples, in particular by reusing the derivatives $\nabla_\psi \log q_\psi(\theta)$ already
% evaluated to form $\nabla_\psi U(\psi)$.


% ==============================================================================

\section{Adversarial variational optimization}

\begin{figure*}
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \caption{Adversarial variational optimization.}

    \begin{flushleft}
        {\it Inputs:} observed data $\{ \mathbf{x}_i \}_{i=1}^N \sim p_r$, simulator $g$.\\
        {\it Outputs:} proposal distribution $q_\psi(\theta)$, such that $p_r \approx p_\psi$.\\
        {\it Hyper-parameters:} The number $n_{\text{critic}}$ of training iterations of $d$; the size $M$ of a mini-batch; the gradient penalty coefficient $\lambda$; the entropy penalty coefficient $\gamma$.
    \end{flushleft}

    \label{alg:avo}
    \begin{algorithmic}[1]
        \State{$q_\psi \leftarrow \text{prior on $\theta$ (with differentiable and known density)}$}
        \While{$\psi$ has not converged}
            \For{$i=1$ to $n_{\text{critic}}$} \Comment{Update $d$}
                \State{Sample a mini-batch $\{\mathbf{x}_m \sim p_r, \theta_m \sim q_\psi, \mathbf{z}_m \sim p_{z(\theta_m)}, \epsilon_m \sim U[0,1] \}_{m=1}^M$.}
                \For{$m=1$ to $M$}
                    \State{$\tilde{\mathbf{x}}_m \leftarrow g(\mathbf{z}_m; \theta_m)$}
                    \State{$\hat{\mathbf{x}}_m \leftarrow \epsilon_m \mathbf{x}_m + (1 - \epsilon_m) \tilde{\mathbf{x}}_m$}
                    \State{$U_d^{(m)} \leftarrow d(\tilde{\mathbf{x}}_m; \phi) - d(\mathbf{x}_m; \phi) + \lambda (|| \nabla_{\hat{\mathbf{x}}_m} d(\hat{\mathbf{x}}_m; \phi) ||_2 - 1)^2 $}
                \EndFor
                \State{$\phi \leftarrow \text{Adam}(\nabla_\phi \tfrac{1}{M} \sum_{m=1}^M U_d^{(m)})$}
            \EndFor
            \State{Sample a mini-batch $\{ \theta_m \sim q_\psi,  \mathbf{z}_m \sim p_{z(\theta_m)} \}_{m=1}^M$.}  \Comment{Update $q_\psi$}
            \State{$\nabla_\psi U_g \leftarrow \tfrac{1}{M} \sum_{m=1}^M -d(g(\mathbf{z}_m; \theta_m)) \nabla_\psi \log q_\psi(\theta_m)$}
            % \State{$\mathbf{F}(\psi) \leftarrow \tfrac{1}{M} \sum_{m=1}^M  \nabla_\psi \log q_\psi(\theta_m) \nabla_\psi \log q_\psi(\theta_m)^{\top}$}
            \State{$\nabla_\psi H(q_\psi) \leftarrow \tfrac{1}{M} \sum_{m=1}^M  \nabla_\psi q_\psi(\theta_m) \log q_\psi(\theta_m)$}
            \State{$\psi \leftarrow \text{Adam}(\nabla_\psi U_g + \gamma \nabla_\psi H(q_\psi))$}
        \EndWhile
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
\end{figure*}

The alternating stochastic gradient descent on ${\cal L}_d$ and ${\cal L}_g$ in
GANs (Section~\ref{sec:gans}) inherently assumes that the generator $g$ is a differentiable function. In
the setting where we are not interested in learning the implicit model itself but are
rather interested in the inference of parameters of a fixed
non-differentiable simulator (Section~\ref{sec:problem}),
gradients $\nabla_\theta g$ either do not exist or cannot be accessed. As a
result, gradients $\nabla_\theta {\cal L}_g$ cannot be constructed and the
optimization procedure cannot be carried out.

In this work, we propose to rely on variational optimization to minimize ${\cal
L}_d$ and ${\cal L}_g$, thereby bypassing the non-differentiability of $g$. More
specifically, we consider a proposal distribution $q_\psi(\theta)$ over the
parameters of $g$ and $p_\theta$ and minimize in alternation the variational upper bounds
\begin{align}
    U_d &= \mathbb{E}_{\theta \sim q_\psi} [ {\cal L}_d ] \label{eqn:vo-ud} \\
    U_g &= \mathbb{E}_{\theta \sim q_\psi} [ {\cal L}_g ] \label{eqn:vo-ug}
\end{align} respectively over $\phi$ and $\psi$.
When updating
$d$, unbiased gradient estimates of $\nabla_\phi U_d$ can be obtained by
evaluating the exact and known gradient of $U_d$ over mini-batches of true and
generated data, as ordinarily done in stochastic gradient descent. When updating
$g$, estimates of $\nabla_\psi U_g$ can be derived with forward
simulations, as described in the previous section.
That is,
\begin{equation}
    \nabla_\psi U_g = \mathbb{E}_{\theta \sim q_\psi(\theta), \mathbf{z} \sim p_{z(\theta)}}  [-d(g(\mathbf{z};\theta);\phi) \nabla_\psi \log q_\psi(\theta)],
\end{equation} which we can approximate with mini-batches of
generated data
\begin{equation}
    \nabla_\psi U_g \approx \frac{1}{M} \sum_{m=1}^M -d(g(\mathbf{z}_m; \theta_m); \phi) \nabla_\psi \log q_\psi(\theta_m)
\end{equation}
for $\theta_m \sim q_\psi$ and $\mathbf{z}_m \sim p_{z(\theta_m)}$ and .
\glnote{Can we exploit the fact that $\nabla_\mathbf{x} d(\mathbf{x})$
is known exactly for building better estimates $\tilde{\nabla}_\psi U_g$?
% In practice, using
% \begin{equation}
%     \tilde{\nabla}_\psi U_g = \frac{1}{M} \sum_{m=1}^M g(\mathbf{z}_m; \theta_m) \nabla_\psi \log q_\psi(\theta_m) \nabla_{\tilde{\mathbf{x}}} (-d(\tilde{\mathbf{x}}_m; \phi))
% \end{equation}
% works as good it seems, but I am not sure to precisely understand why, nor whether this is correct...
% It feels like this is approximating the chain rule $\nabla_\theta g(\theta)  \nabla_{\tilde{\mathbf{x}}} (-d(\tilde{\mathbf{x}}))$.
}
For completeness, Algorithm~\ref{alg:avo} outlines the proposed adversarial variational
optimization procedure, as built on top of WGAN-GP.
Obviously, the variational relaxation could similarly be coupled with
other variants of GANs and/or of evolution strategies.


Practically, the variational objectives \ref{eqn:vo-ud}-\ref{eqn:vo-ug}
have the effect of replacing the modeled data distribution of Eqn.~\ref{eqn:p_theta} with
a distribution parameterized in terms of $\psi$:
\begin{equation}\label{eqn:p_psi}
    \mathbf{x} \sim p_\psi \equiv \theta \sim q_\psi, \mathbf{z} \sim p_{z(\theta)}, \mathbf{x} = g(\mathbf{z}; \theta).
\end{equation}
Intuitively, this corresponds to a family of simulators, each configured
with randomly sampled parameters $\theta \sim q_\psi$, whose joint collection
of generated samples is optimized with adversarial training to approach the real data distribution $p_r$.
More formally, the learned model $p_\psi(\mathbf{x})$ therefore corresponds to the marginal distribution
$\int q_\psi(\theta) p_\theta(\mathbf{x}) d\theta$ of the generated data.
\glnote{Connection with variational bayes?}

In consequence, the proposed inference algorithm does not necessarily guarantee that the
proposal distribution $q_\psi$ will place its mass arbitrarily tight
around the parameters of interest, which might be an issue when one is rather interested in point estimates $\theta^*$.
For this purpose, we augment Eqn.~\ref{eqn:vo-ug}
with a regularization term corresponding to the differential entropy $H$ of
the proposal distribution. That is,
\begin{equation}
    U_g = \mathbb{E}_{\theta \sim q_\psi} [ {\cal L}_g ] + \gamma H(q_\psi)
\end{equation}
where $\gamma \in \mathbb{R}^+$ is a hyperparameter controlling the trade-off
between the generator objective and the tightness of the proposal distribution.
For large values of $\gamma$, the procedure is constrained to fit a proposal
distribution with low entropy, which has the effect of concentrating its density
tightly around one or a few $\theta$ values. On the other hand, for small values of $\gamma$,
proposal distributions with larger entropy are not penalized, which may result
in learning a smeared variation of the original simulator.
\glnote{Add note on minimum entropy, otherwise it degenerates back to a non-differentiable function.}





% ==============================================================================

\section{Experiments}

\subsection{Toy problem}

\subsection{Physics example}


% ==============================================================================

\section{Related works}

As reviewed in \cite{2016arXiv161003483M}, likelihood-free inference is
intimately tied to a class of algorithms that can be framed as density
estimation-by-comparison. In most cases, these algorithms are formulated as an
iterative two-step process where the model distribution is first compared to the
true data distribution and then updated to make it more comparable to the
latter. Closest to our work are inference
algorithms~\citep{Neal:2007zz,gutmann2012noise,goodfellow2014generative,cranmer2015approximating,2016arXiv161110242D}
that rely on a classifier between generated and true data to estimate the
density ratio between the two distributions and make parameter updates.
\glnote{ABC + Classifier ABC.}
\glnote{Explain how this work is different.}





% ==============================================================================

\section{Summary}



% ==============================================================================

\section*{Acknowledgments}

GL and KL are both supported through NSF ACI-1450310, additionally KC is
supported through PHY-1505463 and PHY-1205376.


% ==============================================================================

\bibliographystyle{acm}
\bibliography{bibliography.bib}


\end{document}
